---
title: "Hypothesis testing & permutation"
format: html
---

This week we will introduce the idea of hypothesis testing, p-values, null distributions, and how to compare groups. 

Before we get to the exercises we believe it makes sense to cover in one fell swoop a lot of the theory you need to know for the rest of the materials on this page. 
Watch [Video 1](https://youtu.be/0oc49DyA3hU?si=Q2XAp3fUsce-p4yo){target="_blank"} (*Hypothesis Testing and The Null Hypothesis, Clearly Explained!!!*), [Video 2](https://youtu.be/vemZtEM63GY?si=CWcxU1281x9pwFwx){target="_blank"} (*p-values: What they are and how to interpret them*) and [Video 3](https://youtu.be/N4ZQQqyIf6k?si=A717dyIUzLkUUJH6){target="_blank"} (*Using Bootstrapping to Calculate p-values!!!*).

1. Use your bootstrapping skills to test the hypothesis that the true bill length in Adelie penguins is zero millimeters. 

  * What is the smallest non-zero p-value you can get if you run 10,000 iterations? In statistics we rarely want to say that a probability is zero. Think of a better way to report a p-value that in your calculation ends up being zero. 

  * Run a test investigating the hypothesis that the true bill length in Adelie penguins is 38.5 mm. 

2. A little while ago when we talked about covariance and correlation, we introduced the idea of *chance correlations*; a distribution of correlation coefficients generated by repeatedly sampling pairs of variables for which the true correlation is zero. You can think of this as going out into the real world and taking weight measurements from a sample of individuals and height measurements from another group of people. You can store these vectors side by side in a data frame, but any relationship you might find if you for example calculate the correlation between weights in group 1 and heights in group 2 will of course be due to random chance (hence the term *chance correlation*). If we repeat the process of collecting unrelated data we can create a *null distribution*, and as we already know, and null distribution can be used to calculate *p-values*. 

  * Use the code provided below to generate a correlation null distribution (a distribution of chance correlations). 
  
```{r}
#| eval: false
sim_null_corr <- map(1:10000, \(i) tibble(x = rnorm(20, 0, 10),
                                          y = rnorm(20, 0, 10))) %>% 
  list_rbind(names_to = "iteration")

sim_null_corr %>% 
  group_by(iteration) %>% 
  summarise(null_corr = cor(x, y)) %>% 
  ggplot(aes(x = null_corr)) +
  geom_histogram()
```

  
  * In the code it is possible to change the mean and standard deviation of the populations `x` and `y` are sampled from. How does changing these values affect the null distribution? 
  
  * How large (or more extreme) must a correlation coefficient be for it to be *statistically significant* if the sample size is 10, 100 or 1000? Hint: use the `quantile()` function much like we did last week. 
  
3. Generating null distributions via simulation is a great and powerful tool for statistical inference, but what if you wanted to use a sample of observed values to create a correlation null distribution? We have already seen how this can be done when investigating if a mean of a sample variable is different from zero (or any other value of interest). In this case the mean of the sample was shifted and a null distribution was generated using bootstrapping. It is however not clear how this same method could be use for something like the correlation. Simply shifting the means of the variables we want to compare won't do since the correlation isn't affected by the means. What we want to do instead is to repeatedly break the association between our variables of interest. As we talked about when discussing covariance and correlation, the correlation can be viewed as a measure of how much two variables "rank together", meaning that the order of the observations matter. So by randomly shuffling the order or one (or both) of the variables, the association will be broken and if we do this a bunch of times (each time calculating the correlation) a null distribution will be produced. Let's use this method to generate a null distribution of the correlation between bill length and body mass in Chinstrap penguins. Study and run the code below. 

```{r}
#| eval: false
chinstrap_no_missing <- penguins %>% 
  filter(species == "Chinstrap", 
         !is.na(bill_length_mm),
         !is.na(body_mass_g)) 

break_assoc <- function(data, variable_to_shuffle){
  data %>% 
    mutate(shuffled = sample({{variable_to_shuffle}}, replace = FALSE))
}

iterations <- map(1:10000, \(i) break_assoc(chinstrap_no_missing, bill_length_mm)) %>% 
  list_rbind(names_to = "iteration")

iterations %>% 
  group_by(iteration) %>% 
  summarise(null_corr = cor(shuffled, body_mass_g)) %>% 
  ggplot(aes(x = null_corr)) +
  geom_histogram()
```

* When we bootstrapped, we sampled with replacement (`replace = TRUE`) and in this case we do not. Explain why. 

* Here we only shuffle one variable. Would the results change if we shuffled both `bill_length_mm` and `body_mass_g`? Why/why not? 

* Add the observed correlation to the histogram using `geom_vline()`. If this was a test for which you wanted to report the p-value, what would you report? 

Congratulations! You just performed (perhaps without realizing it) your first *permutation test*. Permutation is a another powerful and useful statistical inference tool. Probably the most common use case for permutation tests is when comparing two groups, often times group means. There is a fantastic online [visual explanation](https://www.jwilber.me/permutationtest/){target="_blank"} of how this works. Go through the webpage, we are quite sure you will enjoy it! 

4. Last week we saw that the confidence intervals for mean bill length in Chinstrap and Gentoo penguins overlapped by a small amount. Josh pointed out that in cases when confidence intervals overlap a little bit, it is still possible that the mean difference is statistically significant. It is now time to test this using permutation. This will be exactly the same idea as what is presented in the alpaca example you just went through. We have already given you all the tools you need to be able to do this yourself. Hint: the `break_assoc()` function will be helpful in this case also and when processing the iterations you may need to group by more than one variable. Reach out to us if you get stuck; it is important that you complete the exercise. 

  * Plot the null distribution and add the observed (initial) mean difference to the plot.
  
  * Calculate the p-value. What conclusions do you draw? 
  
5. Permutation tests are great and we argue that you should use them a lot when analyzing data. Therefore it could be good to have access to some nice helper functions to help facilitate your work. The `infer` package provides such helper functions. Read this [article](https://infer.netlify.app/articles/t_test#sample-t-test-1){target="_blank"} (focus on the 2-sample t-test part, but if you want you can replicate your exercise 1 analysis by following the instructions at the top of the page) and run versions of the analysis you did for exercise 4 using the methods described. 

  * The `shade_p_value()` and `get_p_value()` functions let you specify a `direction =` argument. Run `?shade_p_value()` and find in the help page what options you have regarding direction. Try the different options. How does the output relate to our discussion about *greater than* and *more extreme*? Josh does a good job explaining the difference between one-sided and two-sided p-values in [this Video](https://youtu.be/JQc3yx0-Q9E?si=8E0blQb1Chb4qif1&t=1212){target="_blank"}. There is no need to watch the whole video, just the last part starting at 20:12.  

  * The `t_test()` function from the `infer` package lets you preform a *theory-based t-test*. Use the internet to learn about t-tests. What is a t-statistic and how is it calculated? What is a t-distribution? What are some pros and cons with t-tests vs permutation tests?  

6. We saw above how setting the sd of `x` and `y` affected the width of the null distribution and the effect of increasing sample size on the minimum correlation value to reach significance at a level of 0.05. But when permuting labels, how does the disparity in variances between `x` and `y` affect the significance value you calculate? If you increased the varian

```{r}
#| eval: false
sim_null_corr <- map(1:10000, \(i) tibble(x = rnorm(20, 0, 10),
                                          y = rnorm(20, 0, 100))) %>% 
  list_rbind(names_to = "iteration")

sim_null_corr %>% 
  group_by(iteration) %>% 
  summarise(null_corr = cor(x, y)) %>% 
  ggplot(aes(x = null_corr)) +
  geom_histogram()
```


```{r}
perm_diffs <- map(1:10000, \(i) {
  penguins %>%
    filter(species %in% c("Chinstrap", "Gentoo")) %>%
    break_assoc(bill_length_mm) %>%
    group_by(species) %>%
    summarise(mean_bill = mean(shuffled, na.rm = TRUE)) %>%
    summarise(diff = diff(mean_bill)) %>%
    pull(diff)
}) %>%
  unlist()
```

```{r}
## Inspect null dists of mean diffs with increasing variance. Use p-shade to overlay significance level and find minimally significant threshold

## Changing with sample size - effect of distortion with small, medium and large samples - convergence at asymptoptic samples 

library(tidyverse)

perm_tt <- function(n, sd1, sd2, m1, m2, reps=1000) {
  alpha = 0.05
  dat <- tibble(
    group = rep(c("x","y"), each=n),
    values = c(rnorm(n, m1, sd1), rnorm(n, m2, sd2))
  )

  obs_diff <- dat %>%
    group_by(group) %>%
    summarise(m = mean(values), .groups="drop") %>%
    summarise(diff = diff(m)) %>% 
    pull(diff)
  
  p_ttest <- t.test(values ~ group, data = dat, var.equal=T)$p.value
  
  se <- dat %>% group_by(group) %>% summarize(se_pool = (n - 1) * var(values), .groups="drop") %>% summarize(pooled = sqrt(sum(se_pool)/(n + n - 2))*(1/n + 1/n)) 
  df <- n + n - 2
  tcrit <- qt(1-alpha, df=df)
  thresh_ttest <- tcrit * se
  
  perm_diffs <- map(1:reps, \(i) {
    dat %>%
      break_assoc(values) %>%
      group_by(group) %>%
      summarise(m = mean(shuffled), .groups="drop") %>%
      summarise(diff = diff(m)) %>%
      pull(diff)
  }) %>% as.double() 
  
  p_perm <- mean(abs(perm_diffs) >= abs(obs_diff))
  thresh_perm <- quantile(abs(perm_diffs), probs=1-alpha)
  
  
  list(
    perm_diffs = perm_diffs,
    obs_diff = obs_diff,
    p_ttest = p_ttest,
    thresh_ttest = thresh_ttest,
    p_perm = p_perm,
    thresh_perm = thresh_perm
  )
}

# run scenarios: 

## vary variance difference
iters <- c(5,10,20,50)
change_var <- map(iters, \(x) perm_tt(n=100, sd1=10, sd2=10+x, m1=20, m2=25))

## vary with increasing (but equal between x and y) sample size 
iters <- c(30,50,100,1000)
change_n <- map(iters, \(x) perm_tt(n=x, sd1=10, sd2=30, m1=20, m2=25))

## vary with increasing difference in means
iters <- c(0, 5, 10, 20)
change_means <- map(iters, \(x) perm_tt(n=100, sd1=10, sd2=30, m1=20, m2=25))

# 1. Overlaid density plot of permutation distributions
plot_densities <- function(change,iters) {}

perm_df <- map2(change_var, c(5,10,20,50), \(res, sd_add) {
  tibble(perm_diffs = res$perm_diffs, scenario = paste0("sd2= ", 5+sd_add))
}) %>% list_rbind()

ggplot(perm_df, aes(x=perm_diffs, fill=scenario, color=scenario)) +
  geom_density(alpha=0.3) +
  labs(title="Permutation null distributions under varying variance",
       x="Permutation mean differences", y="Density") +
  theme_minimal()

# 2. Tabulate observed diff, p-values, and differences
results_tab <- map2(change_var, c(5,10,20,50), \(res, sd_add) {
  tibble(
    sd2 = 5+sd_add,
    obs_diff = round(res$obs_diff, 3),
    p_student = signif(res$p_student, 3),
    p_perm = signif(res$p_perm, 3),
    diff_p = round(res$p_perm - res$p_student, 3),
    reject_student = res$p_student < 0.05,
    reject_perm = res$p_perm < 0.05
  )
}) %>% list_rbind()

results_tab

```


7. The output from the `t_test()` function includes the columns `estimate`, `lower_ci` and `upper_ci`. The values under these column names represent the mean difference and confidence interval for the mean difference, respectively. Compare the p-value and confidence interval. How does these compare in terms of statistical significance? 

  * Code up a bootstrapping method that calculates the confidence interval for the mean difference. Hint: you can reuse the `boot()` function from last week, but you may want to add a `group_by()` to it. The trick is to sample with replacement *within* each of the groups you want to compare. 
 